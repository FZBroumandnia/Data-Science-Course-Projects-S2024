{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science - CA#03: *Big data cleaning* - Spring 1403 \\\n",
    "The provided dataset(spotify.parquet) contains information about songs streamed on Spotify, which is an audio streaming and media service provider. You have access to a song’s album, artist, its musical characteristics and its release date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('<style>pre { white-space: pre !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m649.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:13\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m437.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=1473cf1f8549ded8da742ff39e7a268178a1fd9718a05f0ecc46d76f59b607ea\n",
      "  Stored in directory: /home/fzbroumandnia/.cache/pip/wheels/95/13/41/f7f135ee114175605fb4f0a89e7389f3742aa6c1e1a5bcb657\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Warm-up! <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function for showing info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def show_df_info(df: DataFrame, num_rows: int = 5):\n",
    "    row_count = df.count()\n",
    "    print(f\"Number of rows: {row_count}\")\n",
    "    df.show(num_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: read the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, min, max, mean, stddev\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"stocks.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Schema of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Showing opening, closing and volume of Records with closing price less than 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1359\n",
      "+----------+------------------+---------+\n",
      "|      Open|             Close|   Volume|\n",
      "+----------+------------------+---------+\n",
      "|213.429998|        214.009998|123432400|\n",
      "|214.599998|        214.379993|150476200|\n",
      "|214.379993|        210.969995|138040000|\n",
      "|    211.75|            210.58|119282800|\n",
      "|210.299994|211.98000499999998|111902700|\n",
      "+----------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_less_than_500 = df.filter(df[\"Close\"] < 500).select(\"Open\", \"Close\", \"Volume\")\n",
    "show_df_info(df_less_than_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Records with opening price more than 200 and closing price less than 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3\n",
      "+----------+------------------+----------+----------+----------+---------+------------------+\n",
      "|      Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
      "+----------+------------------+----------+----------+----------+---------+------------------+\n",
      "|2010-01-22|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
      "|2010-01-28|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n",
      "+----------+------------------+----------+----------+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_open_more_200_close_less_200 = df.filter((df[\"Open\"] > 200) & (df[\"Close\"] < 200))\n",
    "show_df_info(df_open_more_200_close_less_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Extract year from date and save it in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1762\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|Year|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|2010|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|2010|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|2010|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_year = df.withColumn(\"Year\", year(\"Date\"))\n",
    "show_df_info(df_with_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Minimum volumes traded for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 7\n",
      "+----+---------+\n",
      "|Year|minVolume|\n",
      "+----+---------+\n",
      "|2015| 13046400|\n",
      "|2013| 41888700|\n",
      "|2014| 14479600|\n",
      "|2012| 43938300|\n",
      "|2016| 11475900|\n",
      "|2010| 39373600|\n",
      "|2011| 44915500|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_volume_per_year = df_with_year.groupBy(\"Year\").agg(min(\"Volume\").alias(\"minVolume\"))\n",
    "show_df_info(min_volume_per_year, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Highest low price for each year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 84\n",
      "+----+-----+----------+\n",
      "|Year|Month|    maxLow|\n",
      "+----+-----+----------+\n",
      "|2012|   10|665.550026|\n",
      "|2010|    7|260.300003|\n",
      "|2010|   12|325.099991|\n",
      "|2015|    2|131.169998|\n",
      "|2014|    4|589.799988|\n",
      "+----+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_low_price_per_year_month = df_with_year.groupBy(\"Year\", month(\"Date\").alias(\"Month\")) \\\n",
    "    .agg(max(\"Low\").alias(\"maxLow\"))\n",
    "show_df_info(max_low_price_per_year_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Mean and standard deviation of high price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean High Price: 315.91\n",
      "Standard Deviation of High Price: 186.90\n"
     ]
    }
   ],
   "source": [
    "mean_high_price = df.agg(mean(\"High\").alias(\"MeanHighPrice\")).collect()[0][\"MeanHighPrice\"]\n",
    "stddev_high_price = df.agg(stddev(\"High\").alias(\"StddevHighPrice\")).collect()[0][\"StddevHighPrice\"]\n",
    "print(f\"Mean High Price: {mean_high_price:.2f}\")\n",
    "print(f\"Standard Deviation of High Price: {stddev_high_price:.2f}\")\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Main Task</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import min, max, mean, stddev\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SpotifyAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.load(\"spotify.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- album: string (nullable = true)\n",
      " |-- album_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- artist_ids: string (nullable = true)\n",
      " |-- track_number: long (nullable = true)\n",
      " |-- disc_number: long (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- time_signature: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1204025\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----------+--------+------------------+------+---+-------------------+----+-----------+-----------------+--------------------+--------+------------------+------------------+-----------+--------------+----+------------+\n",
      "|                  id|                name|               album|            album_id|             artists|          artist_ids|track_number|disc_number|explicit|      danceability|energy|key|           loudness|mode|speechiness|     acousticness|    instrumentalness|liveness|           valence|             tempo|duration_ms|time_signature|year|release_date|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----------+--------+------------------+------+---+-------------------+----+-----------+-----------------+--------------------+--------+------------------+------------------+-----------+--------------+----+------------+\n",
      "|7lmeHLHBe4nmXzuXc...|             Testify|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           1|          1|   false|              0.47| 0.978|  7|             -5.399|   1|     0.0727|           0.0261|             1.09E-5|   0.356|             0.503|           117.906|     210133|           4.0|1999|  1999-11-02|\n",
      "|1wsRitfRRtWyEapl0...|     Guerrilla Radio|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           2|          1|    true|             0.599| 0.957| 11| -5.763999999999999|   1|      0.188|           0.0129|             7.06E-5|   0.155|             0.489|            103.68|     206200|           4.0|1999|  1999-11-02|\n",
      "|1hR0fIFK2qRG3f3RF...|    Calm Like a Bomb|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           3|          1|   false|             0.315|  0.97|  7|             -5.424|   1|      0.483|           0.0234|             2.03E-6|   0.122|              0.37|           149.749|     298893|           4.0|1999|  1999-11-02|\n",
      "|2lbASgTSoDO7MTuLA...|           Mic Check|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           4|          1|    true|              0.44| 0.967| 11|              -5.83|   0|      0.237|            0.163|             3.64E-6|   0.121|             0.574|            96.752|     213640|           4.0|1999|  1999-11-02|\n",
      "|1MQTmpYOZ6fcMQc56...|Sleep Now In the ...|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           5|          1|   false|             0.426| 0.929|  2|             -6.729|   1|     0.0701|          0.00162|               0.105|  0.0789|             0.539|           127.059|     205600|           4.0|1999|  1999-11-02|\n",
      "|2LXPNLSMAauNJfnC5...|Born of a Broken Man|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           6|          1|   false|             0.298| 0.848|  2|             -5.947|   1|     0.0727|           0.0538|  0.0015199999999999|   0.201|0.1939999999999999|           148.282|     280960|           4.0|1999|  1999-11-02|\n",
      "|3moeHk8eIajvUEzVo...|      Born As Ghosts|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           7|          1|   false|             0.417| 0.976|  9|             -6.032|   1|      0.175|4.269999999999E-4|             1.34E-4|   0.107|             0.483|            90.395|     202040|           4.0|1999|  1999-11-02|\n",
      "|4llunZfVXv3NvUzXV...|               Maria|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           8|          1|   false|0.2769999999999999| 0.873| 11|-6.5710000000000015|   0|     0.0883|          0.00694|5.400000000000001E-5|   0.188|             0.618|172.84799999999996|     228093|           4.0|1999|  1999-11-02|\n",
      "|21Mq0NzFoVRvOmLTO...|Voice of the Voic...|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           9|          1|   false|             0.441| 0.882|  7| -7.362999999999999|   1|      0.044|           0.0195|             0.00684|    0.15|             0.418| 83.37100000000002|     151573|           4.0|1999|  1999-11-02|\n",
      "|6s2FgJbnnMwFTpWJZ...|New Millennium Homes|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|          10|          1|   false|0.4479999999999999| 0.861|  9|              -6.12|   1|     0.0676|          0.00306|                 0.0|  0.0987|0.7609999999999999|            92.777|     224933|           4.0|1999|  1999-11-02|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----------+--------+------------------+------+---+-------------------+----+-----------+-----------------+--------------------+--------+------------------+------------------+-----------+--------------+----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df_info(df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Questions</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. how Spark and Hadoop work. What does the term ‘lazy evaluation’ mean for them? Explain with a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadoop and Spark Comparison\n",
    "\n",
    "##### Hadoop\n",
    "- Utilizes HDFS for distributed storage and MapReduce for distributed processing.\n",
    "- Well-suited for batch processing of large datasets but can be less efficient for iterative or interactive workloads.\n",
    "\n",
    "##### Spark\n",
    "- Employs in-memory computing and offers various APIs including RDDs, Spark SQL, Spark Streaming, MLlib, and GraphX.\n",
    "- Faster and more versatile than Hadoop, suitable for a wide range of use cases including batch processing, real-time processing, iterative algorithms, and interactive data analysis.\n",
    "\n",
    "#### Lazy Evaluation in Spark and Hadoop\n",
    "\n",
    "In the context of Spark and Hadoop, lazy evaluation means that transformations on data are not immediately executed. Instead, they are stored as a series of transformations to be performed later, when an action is called. This allows for optimization of the computation process.\n",
    "\n",
    "For example, in Spark, if you have a dataset and you apply multiple transformations like filtering, mapping, and aggregating, Spark won't execute these transformations immediately. It will wait until an action like collect() or show() is called. This deferred execution helps Spark optimize the computation by combining multiple transformations and executing them in an efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Your main task’s dataset has about 1,200,000 rows, which makes it quite hard, and even sometimes impossible, to work with. Explain how parquet files try to solve this problem, compared to normal file formats like csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Parquet Files\n",
    "\n",
    "##### Columnar Storage\n",
    "- Parquet files store data in a columnar format, storing each column separately.\n",
    "- Enables efficient compression and encoding techniques on a per-column basis, reducing storage space compared to row-based formats like CSV.\n",
    "\n",
    "##### Predicate Pushdown\n",
    "- Parquet files support predicate pushdown, reading only necessary columns and rows based on query predicates.\n",
    "- Reduces I/O overhead and improves query performance, especially for selective queries.\n",
    "\n",
    "##### Schema Evolution\n",
    "- Parquet files include metadata describing the data schema, allowing for schema evolution without compatibility issues.\n",
    "- New columns can be added or existing columns modified without rewriting the entire dataset.\n",
    "\n",
    "##### Compression\n",
    "- Parquet files support various compression algorithms such as Snappy, GZIP, and LZ4.\n",
    "- Compression techniques reduce storage footprint and improve read/write performance.\n",
    "\n",
    "##### Partitioning\n",
    "- Parquet files can be partitioned based on one or more columns.\n",
    "- Improves query performance by enabling partition pruning, scanning only relevant partitions during query execution.\n",
    "\n",
    "Overall, Parquet files offer significant advantages over traditional file formats like CSV when working with large datasets, making them a preferred choice for big data processing frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sparkdoesn’tsavecheckpoints.How can we enforce it to do so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable Checkpointing\n",
    "\n",
    "Before using checkpointing in Spark, you need to enable it by setting the checkpoint directory using the SparkContext.setCheckpointDir() method. Ensure that this directory is accessible by all nodes in the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"hdfs://path/to/checkpoint/dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Checkpointing\n",
    "\n",
    "Once checkpointing is enabled in Spark, you can enforce it on an RDD or DataFrame by calling the checkpoint() method. This action triggers the execution of the computation graph up to that point and saves the intermediate results to the checkpoint directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.checkpoint()\n",
    "df.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Top companies stream their data on a regular routine, e.g. daily. How can we save data, so that we could filter it based on specific columns, e.g. date, faster than regular filtering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioning by Date\n",
    "\n",
    "If you're streaming data on a daily basis, you can partition the data by date. Each day's data can be saved in a separate directory or file, with the directory or file name representing the date. This allows for fast filtering based on the date column, as Spark or any other processing framework can directly access the relevant partition containing the data for the desired date.\n",
    "\n",
    "#### File Formats\n",
    "\n",
    "Choose a file format that supports partitioning efficiently, such as Parquet. Parquet files can be partitioned based on one or more columns, enabling efficient pruning of irrelevant partitions during query execution.\n",
    "\n",
    "#### Optimized Storage\n",
    "\n",
    "Ensure that the data is stored in a distributed file system like HDFS or an object store like Amazon S3. This ensures that the data is distributed across multiple nodes, allowing for parallel processing and faster retrieval.\n",
    "\n",
    "#### Optimize for Query Patterns\n",
    "\n",
    "Analyze the typical query patterns and partition the data accordingly. For example, if filtering by date is the most common operation, partitioning by date would be beneficial. Similarly, if filtering by another column is more common, consider partitioning by that column instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Let's face off Pandas and PySpark in the data analysis arena! When does each library truly shine, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas vs. PySpark: A Comparative Analysis\n",
    "\n",
    "##### Data Size\n",
    "\n",
    "- Pandas: Best suited for working with small to medium-sized datasets that can fit into memory. It performs well when the dataset fits comfortably on a single machine.\n",
    "- PySpark: Designed for handling large-scale datasets that exceed the memory capacity of a single machine. PySpark distributes the data across a cluster of machines, allowing for processing of massive datasets that cannot be handled by Pandas alone.\n",
    "\n",
    "##### Processing Complexity\n",
    "\n",
    "- Pandas: Ideal for complex data manipulation and analysis tasks due to its rich set of functions and expressive syntax. It offers a wide range of operations for data cleaning, transformation, and analysis, making it suitable for intricate data processing workflows.\n",
    "- PySpark: Well-suited for processing complex data transformations and analytics at scale. PySpark leverages distributed computing to handle complex operations on large datasets efficiently. It excels in scenarios where processing complexity is high and parallel processing is necessary for timely analysis.\n",
    "\n",
    "##### User Experience\n",
    "\n",
    "- Pandas: Offers a user-friendly and intuitive interface, making it easy for data analysts and scientists to perform exploratory data analysis and build models. Pandas' syntax is concise and familiar to Python users, which contributes to its popularity and ease of use.\n",
    "- PySpark: Requires a deeper understanding of distributed computing concepts and Spark's API. While PySpark provides similar functionality to Pandas, transitioning from Pandas to PySpark may require some learning curve, especially for users unfamiliar with distributed computing paradigms. However, once users are comfortable with Spark's API, they can leverage its scalability and performance benefits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
